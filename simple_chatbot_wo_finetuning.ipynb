{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e994e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eccdd0",
   "metadata": {},
   "source": [
    "### Define chat function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbf2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input, chat_history_ids=None):\n",
    "    # Encoding the new user input\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt') \n",
    "    # Generate attention mask \n",
    "    attention_mask = torch.ones_like(new_user_input_ids)\n",
    "\n",
    "    # Appending new user input to chat history\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "        # Update attention mask: 1 for all new user input ids, append 0 for the padding\n",
    "        attention_mask = torch.cat([torch.ones_like(chat_history_ids), torch.ones_like(new_user_input_ids)], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "\n",
    "    # Generating a response\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        attention_mask=attention_mask,  # Pass the attention mask here\n",
    "        max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    # Decode the last output (the model's response)\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    # For debugging: Print the history including the user's input and bot's response\n",
    "    # Uncomment the line below if you want to see the entire token IDs sequence\n",
    "    print(f\"Encoded History: {chat_history_ids.tolist()}\")\n",
    "    print(f\"Decoded History: {tokenizer.decode(chat_history_ids[0], skip_special_tokens=True)}\")\n",
    "\n",
    "    \n",
    "    # Return the chat history and response\n",
    "    return chat_history_ids, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5688a",
   "metadata": {},
   "source": [
    "### Create interactive UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ff9b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d4ffa909d74b03a4b95a9e478a35f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='You:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc0e102d4ad46de8d7190231071e7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048b9d185acc4cc4acb9ea99e95e3f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the interface elements\n",
    "user_input = widgets.Text(value='', placeholder='Type something', description='You:', disabled=False)\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "output = widgets.Output()\n",
    "dialogue = None\n",
    "\n",
    "# Define event when the Send button is clicked\n",
    "def on_send_button_clicked(b):\n",
    "    global dialogue\n",
    "    with output:\n",
    "        clear_output()\n",
    "        # Output the user's message\n",
    "        print(f\"You: {user_input.value}\")\n",
    "        # Call the chat function with user input and existing chat history\n",
    "        dialogue, bot_response = chat(user_input.value, dialogue)\n",
    "        # Output the model's response\n",
    "        print(f\"DialoGPT: {bot_response}\")\n",
    "        # Clear the input for the next message\n",
    "        user_input.value = ''\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "# Display elements\n",
    "display(user_input, send_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e71361c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a767846",
   "metadata": {},
   "source": [
    "## Chatbot Context Truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994c9bd",
   "metadata": {},
   "source": [
    "Language models have a maximum sequence length they can process. For example, GPT-2's maximum context size is 1,024 tokens, and other models may vary. If we keep appending user inputs and bot responses to the conversation history indefinitely, we will eventually exceed the model's maximum input size.\n",
    "\n",
    "To avoid exceeding the maximum length, we must truncate the conversation history by keeping only the most recent relevant parts of the conversation. Below is a code snippet providing an example of how to do this truncation, keeping the conversation within the model's limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0233b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_w_truncation(user_input, chat_history_ids=None):\n",
    "    # Tokenize new user input\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # Combine new input with chat history (if available)\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "        \n",
    "    # Define the maximum token length for the model (e.g., 1024 for GPT-2)\n",
    "    max_length = model.config.n_ctx\n",
    "\n",
    "    # Truncate the concatenated input to fit within the model's maximum input length\n",
    "    if bot_input_ids.shape[-1] > max_length:\n",
    "        bot_input_ids = bot_input_ids[:, -max_length:]\n",
    "    \n",
    "    # Generate attention mask corresponding to the input tokens, ignoring padded tokens\n",
    "    attention_mask = torch.ones_like(bot_input_ids)\n",
    "\n",
    "    # Generate response using model.generate\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "        # Include other generate parameters as needed...\n",
    "    )\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    return chat_history_ids, response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38c5f8",
   "metadata": {},
   "source": [
    "### Chatbot Old Context Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22492ace",
   "metadata": {},
   "source": [
    "To avoid losing potentially relevant information through truncation, we can summarize the older parts of the conversation history instead. Summarization compresses the extended dialogue into a shorter form, retaining the key points. This can be helpful for maintaining context without running into the token limit of the model.\n",
    "\n",
    "Implementing summarization typically involves using a dedicated summarization model that condenses the older context into a shorter version and using this summarized context as a base for further generation. The summarization model should be capable of understanding the narrative of a dialogue and extracting the main topics or intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78224cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_python",
   "language": "python",
   "name": "llm_env_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
