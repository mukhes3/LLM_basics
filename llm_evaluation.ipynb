{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ac765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2c4f4",
   "metadata": {},
   "source": [
    "### Some basics of LLM metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e9f31",
   "metadata": {},
   "source": [
    "\n",
    "The evaluation of language models involves various metrics, each of which provides different insights into the model's performance. Here we describe some of the common metrics used in evaluating language models.\n",
    "\n",
    "#### Perplexity (PPL)\n",
    "\n",
    "- **Mathematical Definition**: $PPL(W) = P(w_1, w_2, \\ldots, w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1, w_2, \\dots, w_N)}}$, where $W$ is the sequence of words and $N$ is the number of words.\n",
    "- **Range**: $[1, \\infty)$.\n",
    "- **Use Cases**: Commonly used to evaluate language models, especially in the context of text generation and next-word prediction.\n",
    "- **Pros**:\n",
    "  - Intuitive interpretation as the weighted average branching factor of the language.\n",
    "  - Lower perplexity indicates a better model (less surprised by the test data).\n",
    "- **Cons**:\n",
    "  - Highly sensitive to data sparsity and may not always correlate with human judgments of text quality.\n",
    "\n",
    "#### BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "- **Mathematical Definition**:  \n",
    "\n",
    "  $$BLEU = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "  where:\n",
    "  - $BP$ is the brevity penalty to penalize short translations.\n",
    "  - $w_n$ is the uniform weight for each $n$-gram (typically $w_n  = \\frac{1}{N}$ for all $n$).\n",
    "  - $p_n$ is the precision of $n$-grams.\n",
    "\n",
    "- **Range**: $[0, 1]$, often multiplied by 100 to give a percentage.\n",
    "- **Use Cases**: Machine translation quality estimation, also used for text summarization and caption generation.\n",
    "- **Pros**:\n",
    "  - Language-independent and easy to understand.\n",
    "  - Correlates well with human judgment at the corpus level.\n",
    "- **Cons**:\n",
    "  - Does not account for meaning or grammatical correctness.\n",
    "  - Heavily reliant on reference translations, which may not encapsulate all valid translations.\n",
    "  \n",
    "An aside, Precision for n-grams is defined as the ratio of the number of matching n-grams to the number of n-grams in the generated sequence.\n",
    "\n",
    "\n",
    "  $$\\text{Precision}_{n} = \\frac{\\sum_{\\text{n-gram} \\in \\text{Hypothesis}} \\min\\left(\\text{Count}(\\text{n-gram}), \\text{Count}_{\\text{Reference}}(\\text{n-gram})\\right)}{\\sum_{\\text{n-gram} \\in \\text{Hypothesis}} \\text{Count}(\\text{n-gram})}$$\n",
    "\n",
    "  where:\n",
    "  - $\\text{Count}(\\text{n-gram})$ is the number of occurrences of the n-gram in the generated sequence (hypothesis).\n",
    "  - $\\text{Count}_{\\text{Reference}}(\\text{n-gram})$ is the number of occurrences of the n-gram in the reference sequence but clipped to the maximum number found in any single reference sequence.\n",
    "\n",
    "N-gram precision is used to assess the overlap between a candidate translation and one or more reference translations, focusing on exact word matches. The clipping is important to prevent a system from getting undue credit for repeated phrases.\n",
    "\n",
    "- **Note**: In practice, for BLEU score calculations, precision is calculated for multiple n-gram lengths (e.g., 1-gram, 2-gram, 3-gram, and 4-gram) and combined using a weighted geometric mean, with brevity penalty incorporated to account for overly short translations.\n",
    "\n",
    "#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "- **Mathematical Definition**: For ROUGE-N:\n",
    "\n",
    "  $$ROUGE\\text{-}N = \\frac{\\sum_{S \\in \\{References\\}} \\sum_{gram_n \\in S} Count_{match}(gram_n)}{\\sum_{S \\in \\{References\\}} \\sum_{gram_n \\in S} Count(gram_n)}$$\n",
    "\n",
    "  where:\n",
    "  - $Count_{match}(gram_n)$ is the number of $n$-grams that appear in both the system output and the reference.\n",
    "  - $Count(gram_n)$ counts the occurrences in the reference summaries.\n",
    "\n",
    "- **Range**: $[0, 1]$.\n",
    "- **Use Cases**: Mainly used for evaluating text summarization and can also be applied to machine translation.\n",
    "- **Pros**:\n",
    "  - Takes into account both the precision and recall, providing a more balanced view of performance.\n",
    "- **Cons**:\n",
    "  - Like BLEU, it is also limited by the quality and variety of reference summaries.\n",
    "\n",
    "#### METEOR (Metric for Evaluation of Translation with Explicit Ordering)\n",
    "\n",
    "- **Mathematical Definition**:  \n",
    "\n",
    "  $$METEOR = \\frac{10P \\cdot R}{R + 9P} \\cdot \\left(1 - \\frac{0.5 \\cdot C}{Unigrams_{test}}\\right)$$\n",
    "\n",
    "  where:\n",
    "  - $P$ is the precision of unigram matches.\n",
    "  - $R$ is the recall of unigram matches.\n",
    "  - $C$ is the number of chunks (contiguous unigram matches) in the alignment.\n",
    "  - $Unigrams_{test}$ is the total number of unigrams in the test output.\n",
    "\n",
    "- **Range**: $[0, 1]$.\n",
    "- **Use Cases**: It is used to evaluate the quality of machine translation outputs.\n",
    "- **Pros**:\n",
    "  - Accounts for word order and synonymy, achieving a better correlation with human judgments.\n",
    "- **Cons**:\n",
    "  - Complex computation with several stages.\n",
    "\n",
    "#### BERTScore\n",
    "\n",
    "BERTScore is an evaluation metric that computes the similarity of contextual embeddings from pre-trained models such as BERT for various aspects of text generation quality.\n",
    "\n",
    "- **Mathematical Definitions**:\n",
    "  - **Precision**: Measures coverage of the candidate's tokens in the reference.\n",
    "    $$ P = \\frac{1}{|C|} \\sum_{i=1}^{|C|} \\max_{j=1}^{|R|} \\text{cos}(c_i, r_j) $$\n",
    "  - **Recall**: Measures coverage of the reference's tokens in the candidate.\n",
    "    $$ R = \\frac{1}{|R|} \\sum_{i=1}^{|R|} \\max_{j=1}^{|C|} \\text{cos}(r_i, c_j) $$\n",
    "  - **F1 Score**: The harmonic mean of precision and recall.\n",
    "    $$ F1 = \\frac{2 \\cdot P \\cdot R}{P + R} $$\n",
    "\n",
    "  where:\n",
    "  - $|C|$ is the number of tokens in the candidate (generated) text.\n",
    "  - $|R|$ is the number of tokens in the reference text.\n",
    "  - $c_i$ is the embedding of the $i$-th token in the candidate text.\n",
    "  - $r_j$ is the embedding of the $j$-th token in the reference text.\n",
    "  - $\\text{cos}$ denotes the cosine similarity function.\n",
    "\n",
    "- **Range**:\n",
    "  - **Precision** and **Recall**: Typically $[0, 1]$ with 1 indicating perfect precision or recall.\n",
    "  - **F1 Score**: Also typically $[0, 1]$ with 1 being the best F1 score.\n",
    "\n",
    "- **Use Cases**: BERTScore is used for evaluating the quality of generated text in tasks such as translation, summarization, text generation, and more. It provides individual measurements for precision, recall, and F1, offering a multifaceted view of a modelâ€™s performance.\n",
    "\n",
    "- **Pros**:\n",
    "  - Provides a more nuanced evaluation by computing separate scores for precision, recall, and F1.\n",
    "  - Captures semantic similarity better than overlap-based metrics, like BLEU.\n",
    "  - Robust to paraphrasing and more sensitive to the meaning of the text.\n",
    "\n",
    "- **Cons**:\n",
    "  - Computationally intensive, as it uses contextual embeddings from large transformer models.\n",
    "  - The high-resource requirement to run evaluations, especially with large datasets.\n",
    "  - The need for careful selection of baseline or reference models to ensure fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1c74c",
   "metadata": {},
   "source": [
    "## Load a small evaluation dataset and subsample to 100 reference texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624da7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('cnn_dailymail', '3.0.0', split='validation')\n",
    "dataset = dataset.shuffle(seed=42).select(range(10)) # Subsampling to 100 reference texts\n",
    "\n",
    "# Preview the structure of the dataset\n",
    "print(dataset.column_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8cb0f0",
   "metadata": {},
   "source": [
    "## Load 2 small LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aaed919",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"sshleifer/distilbart-cnn-6-6\", \"sshleifer/distilbart-cnn-12-6\"]\n",
    "models = []\n",
    "tokenizers = []\n",
    "\n",
    "# Load models and tokenizers\n",
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) # Ensure using fast tokenizers\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "    models.append(model)\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db2b9a",
   "metadata": {},
   "source": [
    "## Define function for evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a09a20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, model_names, tokenizers, dataset, metrics):\n",
    "    results = {}\n",
    "    for model, tokenizer, name in zip(models, tokenizers, model_names):\n",
    "        model.to('cuda') # Move model to GPU\n",
    "        \n",
    "        model_results = {}\n",
    "        for metric_name in metrics:\n",
    "            metric = evaluate.load(metric_name)\n",
    "            generated_texts = []\n",
    "            references = []\n",
    "            for example in dataset:\n",
    "                # Make sure to generate text using the model and move it to the GPU\n",
    "                input_text = example['article']  # Article text\n",
    "                reference = example['highlights']  # Associated highlights or summary\n",
    "                \n",
    "                # Tokenize the input and generate the summary outputs\n",
    "                inputs = tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "                inputs = inputs.to('cuda')  # Move tokenized inputs to GPU\n",
    "                summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "                generated = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                generated_texts.append(generated)\n",
    "                references.append(reference)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if metric_name in ['bleu', 'rouge']:\n",
    "                # For `bleu` and `rouge`, references are expected to be lists of lists of strings\n",
    "                metric_result = metric.compute(predictions=generated_texts, references=[[r] for r in references])\n",
    "            elif metric_name == 'bertscore':\n",
    "                # `bertscore` expects lists of strings for both predictions and references\n",
    "                metric_result = metric.compute(predictions=generated_texts, references=references,  lang='en', device='cuda')\n",
    "            else:\n",
    "                raise ValueError(\"Metric not supported\")\n",
    "            \n",
    "            model_results[metric_name] = metric_result\n",
    "        results[name] = model_results\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d472d0a",
   "metadata": {},
   "source": [
    "### Generate evaluation table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fff9a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metrics = ['rouge', 'bertscore', 'bleu'] # BLEU is less common for summarization tasks, so you might skip it\n",
    "evaluation_results = evaluate_models(models, model_names, tokenizers, dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9243be6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">rouge</th>\n",
       "      <th colspan=\"4\" halign=\"left\">bertscore</th>\n",
       "      <th colspan=\"6\" halign=\"left\">bleu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>f1</th>\n",
       "      <th>hashcode</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>bleu</th>\n",
       "      <th>brevity_penalty</th>\n",
       "      <th>length_ratio</th>\n",
       "      <th>precisions</th>\n",
       "      <th>reference_length</th>\n",
       "      <th>translation_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sshleifer/distilbart-cnn-6-6</th>\n",
       "      <td>0.384050</td>\n",
       "      <td>0.151075</td>\n",
       "      <td>0.248238</td>\n",
       "      <td>0.317526</td>\n",
       "      <td>[0.9208353161811829, 0.8709611296653748, 0.850...</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "      <td>[0.9184439778327942, 0.8651304244995117, 0.853...</td>\n",
       "      <td>[0.9232390522956848, 0.8768709897994995, 0.848...</td>\n",
       "      <td>0.126661</td>\n",
       "      <td>0.915957</td>\n",
       "      <td>0.919298</td>\n",
       "      <td>[0.41412213740458015, 0.1575875486381323, 0.08...</td>\n",
       "      <td>570</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sshleifer/distilbart-cnn-12-6</th>\n",
       "      <td>0.446609</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.322850</td>\n",
       "      <td>0.375602</td>\n",
       "      <td>[0.9448792934417725, 0.916256844997406, 0.8688...</td>\n",
       "      <td>roberta-large_L17_no-idf_version=0.3.12(hug_tr...</td>\n",
       "      <td>[0.9340245723724365, 0.9070568680763245, 0.864...</td>\n",
       "      <td>[0.9559891819953918, 0.9256454110145569, 0.873...</td>\n",
       "      <td>0.181944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.114035</td>\n",
       "      <td>[0.4393700787401575, 0.2, 0.13008130081300814,...</td>\n",
       "      <td>570</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  rouge                                \\\n",
       "                                 rouge1    rouge2    rougeL rougeLsum   \n",
       "sshleifer/distilbart-cnn-6-6   0.384050  0.151075  0.248238  0.317526   \n",
       "sshleifer/distilbart-cnn-12-6  0.446609  0.219800  0.322850  0.375602   \n",
       "\n",
       "                                                                       bertscore  \\\n",
       "                                                                              f1   \n",
       "sshleifer/distilbart-cnn-6-6   [0.9208353161811829, 0.8709611296653748, 0.850...   \n",
       "sshleifer/distilbart-cnn-12-6  [0.9448792934417725, 0.916256844997406, 0.8688...   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                        hashcode   \n",
       "sshleifer/distilbart-cnn-6-6   roberta-large_L17_no-idf_version=0.3.12(hug_tr...   \n",
       "sshleifer/distilbart-cnn-12-6  roberta-large_L17_no-idf_version=0.3.12(hug_tr...   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                       precision   \n",
       "sshleifer/distilbart-cnn-6-6   [0.9184439778327942, 0.8651304244995117, 0.853...   \n",
       "sshleifer/distilbart-cnn-12-6  [0.9340245723724365, 0.9070568680763245, 0.864...   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                          recall   \n",
       "sshleifer/distilbart-cnn-6-6   [0.9232390522956848, 0.8768709897994995, 0.848...   \n",
       "sshleifer/distilbart-cnn-12-6  [0.9559891819953918, 0.9256454110145569, 0.873...   \n",
       "\n",
       "                                   bleu                               \\\n",
       "                                   bleu brevity_penalty length_ratio   \n",
       "sshleifer/distilbart-cnn-6-6   0.126661        0.915957     0.919298   \n",
       "sshleifer/distilbart-cnn-12-6  0.181944        1.000000     1.114035   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                      precisions   \n",
       "sshleifer/distilbart-cnn-6-6   [0.41412213740458015, 0.1575875486381323, 0.08...   \n",
       "sshleifer/distilbart-cnn-12-6  [0.4393700787401575, 0.2, 0.13008130081300814,...   \n",
       "\n",
       "                                                                   \n",
       "                              reference_length translation_length  \n",
       "sshleifer/distilbart-cnn-6-6               570                524  \n",
       "sshleifer/distilbart-cnn-12-6              570                635  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, iterate through evaluation results to structure the dictionary as needed beforehand\n",
    "structured_results = {}\n",
    "for model_name in evaluation_results:\n",
    "    for metric in metrics:  # 'rouge', 'bertscore', 'bleu'\n",
    "        for key, value in evaluation_results[model_name][metric].items():\n",
    "            # Add an entry for each sub-metric for each model\n",
    "            structured_results[(metric, key)] = structured_results.get((metric, key), []) + [value]\n",
    "\n",
    "# Now we create a DataFrame with the structured results\n",
    "evaluation_table = pd.DataFrame(structured_results, index=model_names)\n",
    "\n",
    "# If you want to reorder the level of columns you can do so as follows\n",
    "# Let's say you wanted 'rouge-1', 'rouge-2', 'rouge-L', etc. under 'rouge'\n",
    "# and 'precision', 'recall', 'f1' under 'bertscore'.\n",
    "evaluation_table = evaluation_table.reindex(columns=pd.MultiIndex.from_tuples(\n",
    "    [(metric, sub_metric) for metric in metrics for sub_metric in sorted(evaluation_table.columns.get_level_values(1))\n",
    "     if (metric, sub_metric) in evaluation_table.columns]\n",
    "), fill_value=0)\n",
    "\n",
    "# Show the evaluation table with metrics as top-level headers and sub-metrics as secondary headers\n",
    "display(evaluation_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31b1b2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199981bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_python",
   "language": "python",
   "name": "llm_env_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
