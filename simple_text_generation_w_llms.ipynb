{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8352ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db498c8",
   "metadata": {},
   "source": [
    "### Create function for generating text given a prompt and output text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b295f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt_text, max_new_tokens=50):\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "    # GPT-2 tokenizers do not have a pad token by default. Set it to eos_token.\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Encode the input text to get token IDs\n",
    "    encoded_input = tokenizer(prompt_text, return_tensors='pt')\n",
    "\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "    # Generate a sequence of new tokens after the prompt\n",
    "    output_list = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,  # Set the number of new tokens to generate\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the output token IDs to a string\n",
    "    generated_sequence = output_list[0]\n",
    "    text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f897c4",
   "metadata": {},
   "source": [
    "### Try out some very basic text generation given a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88b021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical genetics is a subfield of human genetics. It is the basis of the theory that genetics can be used to predict the genetic makeup of humans.\n",
      "\n",
      "The genetic analysis of genes is based on the assumption that the genes are related to the environment. The genetic data are based upon the hypothesis that genes can have a role in the development of a particular type of genetic trait. This hypothesis is supported by the fact that genetic variation in genes may be related with the presence of other genetic factors. In addition, the data on genes in humans are not based solely on genetic information. Genetic variation is not a factor in human genetic development. However, genetic variations in genetic traits are also related. For example, in a human population, a genetic mutation can cause a mutation that causes a change in gene expression. Therefore, it is possible that a gene mutation may cause an increase in expression of certain genes. Thus, there is no evidence that an increased expression in certain gene variants may have an effect on human development or development in other areas. Furthermore,\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "prompt = \"Statistical genetics is a subfield of human genetics\"\n",
    "generated_text = generate_text(prompt, max_new_tokens=200)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3cfa3",
   "metadata": {},
   "source": [
    "### Exploring the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6cd0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Token', 'ization', 'Ġis', 'Ġessential', 'Ġfor', 'Ġnatural', 'Ġlanguage', 'Ġprocessing', '.']\n",
      "Token IDs: [30642, 1634, 318, 6393, 329, 3288, 3303, 7587, 13]\n",
      "Back to Tokens: ['Token', 'ization', 'Ġis', 'Ġessential', 'Ġfor', 'Ġnatural', 'Ġlanguage', 'Ġprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "sample_text = \"Tokenization is essential for natural language processing.\"\n",
    "\n",
    "# Tokenize text into tokens\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f'Tokens: {tokens}')\n",
    "\n",
    "# Convert tokens to their respective IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "# Convert IDs back to tokens\n",
    "back_to_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f'Back to Tokens: {back_to_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e0a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input with special tokens (IDs): [30642, 1634, 318, 6393, 329, 3288, 3303, 7587, 13]\n",
      "Decoded Text: Tokenization is essential for natural language processing.\n"
     ]
    }
   ],
   "source": [
    "# Encode text (tokens to IDs with additional handling of special tokens)\n",
    "encoded_input = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(f'Encoded Input with special tokens (IDs): {encoded_input}')\n",
    "\n",
    "# Decode the encoded IDs back to text\n",
    "decoded_text = tokenizer.decode(encoded_input)\n",
    "print(f'Decoded Text: {decoded_text}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c38230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Plus with Padding and Attention Mask: {'input_ids': tensor([[30642,  1634,   318,  6393,   329,  3288,  3303,  7587,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])}\n",
      "Special Tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate padding, truncation, and attention mask\n",
    "# Padding ensures that sequences are the same length, attention masks tell the model which tokens to pay attention to\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "encoded_plus = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    max_length=30,         # Pad or truncate to this length\n",
    "    padding='max_length',  # Add padding\n",
    "    truncation=True,       # Enable truncation to max_length\n",
    "    return_tensors='pt',   # Return PyTorch tensors\n",
    "    return_attention_mask=True\n",
    ")\n",
    "print(f'Encoded Plus with Padding and Attention Mask: {encoded_plus}')\n",
    "\n",
    "# Let's see how special tokens work:\n",
    "print(f\"Special Tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "print(encoded_plus['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99914278",
   "metadata": {},
   "source": [
    "### Understanding the model.generate() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82b590",
   "metadata": {},
   "source": [
    "When using language models for text generation, two parameters that influence the diversity of the generated text are `top_k` and `top_p`. These parameters control the sampling process that the model uses to pick the next word.\n",
    "\n",
    "#### top_k Sampling\n",
    "\n",
    "- `top_k` sampling limits the model's choice for the next token to the top-k most likely options out of the probability distribution given by the model. The value of `top_k` could range anywhere from 1 to the size of the model's vocabulary.\n",
    "- A `top_k` value of `1` (greedy sampling) means the model will always choose the most probable token, which tends to produce repetitive and deterministic output.\n",
    "- As `top_k` increases, the model's choices become more diverse, and the generated text becomes more varied and less predictable.\n",
    "- However, too high a `top_k` can also include low-probability words, leading to nonsensical results.\n",
    "\n",
    "#### top_p (Nucleus) Sampling\n",
    "\n",
    "- `top_p` sampling, also known as nucleus sampling, introduces a dynamic approach where the model considers a subset of the vocabulary whose cumulative probability exceeds the threshold `p`.\n",
    "- The `top_p` value is a float between 0 and 1 that defines the probability mass to cover. For example, `top_p=0.9` means the model will sample from the smallest set of tokens that have a combined probability over 90%.\n",
    "- Unlike `top_k`, `top_p` adapts to the token's probability distribution: sometimes it may consider more tokens if the distribution is flat, or fewer tokens if there's a sharp drop-off in probability.\n",
    "\n",
    "#### Comparison and Interaction\n",
    "\n",
    "- `top_k` and `top_p` can be used together, providing a way to manage the trade-off between diversity and relevance of the generated text.\n",
    "- When both are used, the model first filters the top-k tokens and then within this subset applies `top_p` sampling to ensure the cumulative probability meets the threshold.\n",
    "\n",
    "By fine-tuning the `top_k` and `top_p` parameters, one can control the randomness and creativity of the generated text while still maintaining coherence and relevancy to the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17550d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_python",
   "language": "python",
   "name": "llm_env_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
